"""
N√≥s do Grafo - Implementa√ß√£o da l√≥gica de cada etapa
"""
from typing import Any, Dict, Optional
from datetime import datetime
from .states import ResearchState, SearchResult, ValidationResult
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
import json
import os
from dotenv import load_dotenv

# Carrega vari√°veis de ambiente
load_dotenv()


class ResearchNodes:
    """Implementa√ß√£o de todos os n√≥s do grafo de pesquisa"""

    def __init__(self, api_key: Optional[str] = None, tavily_api_key: Optional[str] = None):
        """Inicializa os n√≥s com as APIs necess√°rias"""
        self.llm = ChatAnthropic(
            model="claude-3-haiku-20240307",
            api_key=api_key or os.getenv("ANTHROPIC_API_KEY"),
            temperature=0.3
        )

        # Para busca web, vamos usar Tavily (voc√™ pode substituir por outra API)
        self.tavily_key = tavily_api_key or os.getenv("TAVILY_API_KEY")

    def plan_research(self, state: ResearchState) -> Dict[str, Any]:
        """
        N√≥ inicial: planeja a pesquisa e gera queries de busca
        """
        print(f"\nüéØ PLANEJANDO PESQUISA: {state['query']}")

        prompt = f"""Voc√™ √© um pesquisador experiente. Dada a seguinte pergunta de pesquisa,
gere 3-5 queries de busca espec√≠ficas e complementares que ajudar√£o a obter uma resposta completa.

Pergunta: {state['query']}

Retorne apenas as queries, uma por linha, sem numera√ß√£o ou formata√ß√£o extra."""

        response = self.llm.invoke([
            SystemMessage(content="Voc√™ √© um assistente de pesquisa expert."),
            HumanMessage(content=prompt)
        ])

        queries = [q.strip() for q in response.content.strip().split('\n') if q.strip()]

        return {
            "search_queries": queries,
            "messages": [f"Planejamento completo: {len(queries)} queries geradas"],
            "current_iteration": 0
        }

    def search_web(self, state: ResearchState) -> Dict[str, Any]:
        """
        N√≥ de busca: executa as queries e coleta resultados
        """
        print(f"\nüîç BUSCANDO INFORMA√á√ïES ({len(state.get('search_queries', []))} queries)")

        search_results = []

        # Verifica se deve usar Tavily ou simula√ß√£o
        use_tavily = self.tavily_key and self.tavily_key != "sua-chave-tavily-aqui"

        if use_tavily:
            # Busca REAL com Tavily API
            try:
                from tavily import TavilyClient
                tavily_client = TavilyClient(api_key=self.tavily_key)
                print("  üåê Usando Tavily API (busca real)")

                for query in state.get('search_queries', []):
                    try:
                        # Busca real
                        response = tavily_client.search(
                            query=query,
                            search_depth="basic",
                            max_results=3
                        )

                        # Processa resultados
                        for item in response.get('results', []):
                            result = SearchResult(
                                source=item.get('url', 'N/A'),
                                title=item.get('title', query),
                                content=item.get('content', ''),
                                relevance_score=item.get('score', 0.5),
                                timestamp=datetime.now().isoformat()
                            )
                            search_results.append(result)

                        print(f"  ‚úì Busca real: {query[:60]}... ({len(response.get('results', []))} resultados)")

                    except Exception as e:
                        print(f"  ‚ö†Ô∏è  Erro na busca '{query}': {e}")
                        # Fallback para simula√ß√£o em caso de erro
                        result = self._simulate_search(query)
                        search_results.append(result)

            except ImportError:
                print("  ‚ö†Ô∏è  Tavily n√£o instalado, usando simula√ß√£o")
                use_tavily = False

        if not use_tavily:
            # Simula√ß√£o com LLM
            print("  ü§ñ Usando simula√ß√£o com LLM")
            for query in state.get('search_queries', []):
                result = self._simulate_search(query)
                search_results.append(result)
                print(f"  ‚úì Busca simulada: {query[:60]}...")

        return {
            "search_results": search_results,
            "messages": [f"Busca completa: {len(search_results)} resultados coletados"]
        }

    def _simulate_search(self, query: str) -> SearchResult:
        """Simula√ß√£o de busca com LLM"""
        prompt = f"""Simule um resultado de busca para: "{query}"

Forne√ßa informa√ß√µes factuais e realistas sobre este t√≥pico.
Seja espec√≠fico e inclua detalhes verific√°veis."""

        response = self.llm.invoke([
            SystemMessage(content="Voc√™ √© um motor de busca que retorna informa√ß√µes factuais."),
            HumanMessage(content=prompt)
        ])

        return SearchResult(
            source=f"fonte-simulada-{hash(query) % 1000}.com",
            title=f"Resultado para: {query[:50]}",
            content=response.content,
            relevance_score=0.85,
            timestamp=datetime.now().isoformat()
        )

    def validate_information(self, state: ResearchState) -> Dict[str, Any]:
        """
        N√≥ de valida√ß√£o: cruza informa√ß√µes e detecta conflitos
        """
        print(f"\n‚úÖ VALIDANDO INFORMA√á√ïES")

        results = state.get('search_results', [])
        if not results:
            return {"validations": [], "conflicts_detected": False}

        # Extrai claims principais de cada resultado
        all_content = "\n\n---\n\n".join([
            f"FONTE {i+1} ({r.source}):\n{r.content}"
            for i, r in enumerate(results)
        ])

        prompt = f"""Analise as seguintes informa√ß√µes de m√∫ltiplas fontes sobre: "{state['query']}"

{all_content}

Sua tarefa:
1. Identifique as principais afirma√ß√µes/claims
2. Verifique se h√° consenso entre as fontes
3. Detecte conflitos ou contradi√ß√µes
4. Avalie a confiabilidade de cada afirma√ß√£o

Retorne um JSON com este formato:
{{
  "validations": [
    {{
      "claim": "afirma√ß√£o identificada",
      "is_validated": true/false,
      "confidence": 0.0-1.0,
      "supporting_sources": ["fonte1", "fonte2"],
      "conflicting_info": "descri√ß√£o de conflitos se houver",
      "reasoning": "explica√ß√£o da valida√ß√£o"
    }}
  ],
  "conflicts_detected": true/false,
  "summary": "resumo da valida√ß√£o"
}}"""

        response = self.llm.invoke([
            SystemMessage(content="Voc√™ √© um analista de informa√ß√µes que valida fatos cruzando fontes."),
            HumanMessage(content=prompt)
        ])

        # Parse da resposta
        try:
            # Extrai JSON da resposta
            content = response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            validation_data = json.loads(content)

            validations = [
                ValidationResult(**v) for v in validation_data.get('validations', [])
            ]

            conflicts = validation_data.get('conflicts_detected', False)

            print(f"  ‚úì {len(validations)} afirma√ß√µes validadas")
            if conflicts:
                print(f"  ‚ö†Ô∏è  Conflitos detectados!")

            return {
                "validations": validations,
                "conflicts_detected": conflicts,
                "messages": [validation_data.get('summary', 'Valida√ß√£o completa')]
            }

        except json.JSONDecodeError as e:
            print(f"  ‚ö†Ô∏è  Erro ao parsear valida√ß√£o: {e}")
            return {
                "validations": [],
                "conflicts_detected": False,
                "messages": ["Erro na valida√ß√£o - usando resposta como texto"],
                "error": str(e)
            }

    def synthesize_report(self, state: ResearchState) -> Dict[str, Any]:
        """
        N√≥ de s√≠ntese: cria relat√≥rio final com refer√™ncias
        """
        print(f"\nüìù SINTETIZANDO RELAT√ìRIO FINAL")

        results = state.get('search_results', [])
        validations = state.get('validations', [])

        # Prepara contexto para s√≠ntese
        sources_summary = "\n\n".join([
            f"FONTE {i+1}: {r.source}\n{r.content[:500]}..."
            for i, r in enumerate(results)
        ])

        validations_summary = "\n".join([
            f"- {v.claim} (confian√ßa: {v.confidence:.0%})"
            for v in validations
        ])

        prompt = f"""Com base na pesquisa realizada sobre "{state['query']}", crie um relat√≥rio final completo.

FONTES CONSULTADAS:
{sources_summary}

VALIDA√á√ïES:
{validations_summary}

Crie um relat√≥rio que:
1. Responda diretamente √† pergunta original
2. Apresente as informa√ß√µes validadas
3. Mencione incertezas ou conflitos encontrados
4. Cite as fontes apropriadamente
5. Indique o n√≠vel de confian√ßa geral

Formato do relat√≥rio: Markdown profissional"""

        response = self.llm.invoke([
            SystemMessage(content="Voc√™ √© um pesquisador acad√™mico que escreve relat√≥rios claros e bem referenciados."),
            HumanMessage(content=prompt)
        ])

        # Calcula confian√ßa m√©dia
        avg_confidence = sum(v.confidence for v in validations) / len(validations) if validations else 0.5

        # Prepara refer√™ncias
        references = [
            {
                "source": r.source,
                "title": r.title,
                "url": r.source
            }
            for r in results
        ]

        print(f"  ‚úì Relat√≥rio gerado (confian√ßa: {avg_confidence:.0%})")

        return {
            "final_report": response.content,
            "references": references,
            "confidence_level": avg_confidence,
            "messages": ["S√≠ntese completa"]
        }

    def decide_next_step(self, state: ResearchState) -> str:
        """
        N√≥ de decis√£o: determina se precisa de mais pesquisa
        """
        current_iter = state.get('current_iteration', 0)
        max_iter = state.get('max_iterations', 1)
        conflicts = state.get('conflicts_detected', False)
        validations = state.get('validations', [])

        # Se tem conflitos e ainda tem itera√ß√µes dispon√≠veis
        if conflicts and current_iter < max_iter:
            print(f"\nüîÑ Conflitos detectados - Nova itera√ß√£o necess√°ria")
            return "research_more"

        # Se tem poucas valida√ß√µes e ainda pode iterar
        if len(validations) < 3 and current_iter < max_iter:
            print(f"\nüîÑ Informa√ß√µes insuficientes - Nova itera√ß√£o necess√°ria")
            return "research_more"

        print(f"\n‚úÖ Pesquisa completa - Gerando relat√≥rio final")
        return "synthesize"