"""
NÃ³s do Grafo - ImplementaÃ§Ã£o da lÃ³gica de cada etapa
"""
from typing import Any, Dict, Optional
from datetime import datetime
from .states import ResearchState, SearchResult, ValidationResult
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
import json
import os
from dotenv import load_dotenv

# Carrega variÃ¡veis de ambiente
load_dotenv()


class ResearchNodes:
    """ImplementaÃ§Ã£o de todos os nÃ³s do grafo de pesquisa"""

    def __init__(self, api_key: Optional[str] = None, tavily_api_key: Optional[str] = None):
        """Inicializa os nÃ³s com as APIs necessÃ¡rias"""
        self.llm = ChatAnthropic(
            model="claude-3-haiku-20240307",
            api_key=api_key or os.getenv("ANTHROPIC_API_KEY"),
            temperature=0.3
        )

        # Para busca web, vamos usar Tavily (vocÃª pode substituir por outra API)
        self.tavily_key = tavily_api_key or os.getenv("TAVILY_API_KEY")

    def plan_research(self, state: ResearchState) -> Dict[str, Any]:
        """
        NÃ³ inicial: planeja a pesquisa e gera queries de busca
        """
        print(f"\nğŸ¯ PLANEJANDO PESQUISA: {state['query']}")

        prompt = f"""VocÃª Ã© um pesquisador experiente. Dada a seguinte pergunta de pesquisa,
gere 3-5 queries de busca especÃ­ficas e complementares que ajudarÃ£o a obter uma resposta completa.

Pergunta: {state['query']}

Retorne apenas as queries, uma por linha, sem numeraÃ§Ã£o ou formataÃ§Ã£o extra."""

        response = self.llm.invoke([
            SystemMessage(content="VocÃª Ã© um assistente de pesquisa expert."),
            HumanMessage(content=prompt)
        ])

        queries = [q.strip() for q in response.content.strip().split('\n') if q.strip()]

        return {
            "search_queries": queries,
            "messages": [f"Planejamento completo: {len(queries)} queries geradas"],
            "current_iteration": 0
        }

    def search_web(self, state: ResearchState) -> Dict[str, Any]:
        """
        NÃ³ de busca: executa as queries e coleta resultados
        """
        print(f"\nğŸ” BUSCANDO INFORMAÃ‡Ã•ES ({len(state.get('search_queries', []))} queries)")

        search_results = []

        # Verifica se deve usar Tavily ou simulaÃ§Ã£o
        use_tavily = self.tavily_key and self.tavily_key != "sua-chave-tavily-aqui"

        if use_tavily:
            # Busca REAL com Tavily API
            try:
                from tavily import TavilyClient
                tavily_client = TavilyClient(api_key=self.tavily_key)
                print("  ğŸŒ Usando Tavily API (busca real)")

                for query in state.get('search_queries', []):
                    try:
                        # Busca real
                        response = tavily_client.search(
                            query=query,
                            search_depth="basic",
                            max_results=3
                        )

                        # Processa resultados
                        for item in response.get('results', []):
                            result = SearchResult(
                                source=item.get('url', 'N/A'),
                                title=item.get('title', query),
                                content=item.get('content', ''),
                                relevance_score=item.get('score', 0.5),
                                timestamp=datetime.now().isoformat()
                            )
                            search_results.append(result)

                        print(f"  âœ“ Busca real: {query[:60]}... ({len(response.get('results', []))} resultados)")

                    except Exception as e:
                        print(f"  âš ï¸  Erro na busca '{query}': {e}")
                        # Fallback para simulaÃ§Ã£o em caso de erro
                        result = self._simulate_search(query)
                        search_results.append(result)

            except ImportError:
                print("  âš ï¸  Tavily nÃ£o instalado, usando simulaÃ§Ã£o")
                use_tavily = False

        if not use_tavily:
            # SimulaÃ§Ã£o com LLM
            print("  ğŸ¤– Usando simulaÃ§Ã£o com LLM")
            for query in state.get('search_queries', []):
                result = self._simulate_search(query)
                search_results.append(result)
                print(f"  âœ“ Busca simulada: {query[:60]}...")

        return {
            "search_results": search_results,
            "messages": [f"Busca completa: {len(search_results)} resultados coletados"]
        }

    def _simulate_search(self, query: str) -> SearchResult:
        """SimulaÃ§Ã£o de busca com LLM"""
        prompt = f"""Simule um resultado de busca para: "{query}"

ForneÃ§a informaÃ§Ãµes factuais e realistas sobre este tÃ³pico.
Seja especÃ­fico e inclua detalhes verificÃ¡veis."""

        response = self.llm.invoke([
            SystemMessage(content="VocÃª Ã© um motor de busca que retorna informaÃ§Ãµes factuais."),
            HumanMessage(content=prompt)
        ])

        return SearchResult(
            source=f"fonte-simulada-{hash(query) % 1000}.com",
            title=f"Resultado para: {query[:50]}",
            content=response.content,
            relevance_score=0.85,
            timestamp=datetime.now().isoformat()
        )

    def validate_information(self, state: ResearchState) -> Dict[str, Any]:
        """
        NÃ³ de validaÃ§Ã£o: cruza informaÃ§Ãµes e detecta conflitos
        """
        print(f"\nâœ… VALIDANDO INFORMAÃ‡Ã•ES")

        results = state.get('search_results', [])
        if not results:
            return {"validations": [], "conflicts_detected": False}

        # Extrai claims principais de cada resultado
        all_content = "\n\n---\n\n".join([
            f"FONTE {i+1} ({r.source}):\n{r.content}"
            for i, r in enumerate(results)
        ])

        prompt = f"""Analise as seguintes informaÃ§Ãµes de mÃºltiplas fontes sobre: "{state['query']}"

{all_content}

Sua tarefa:
1. Identifique as principais afirmaÃ§Ãµes/claims
2. Verifique se hÃ¡ consenso entre as fontes
3. Detecte conflitos ou contradiÃ§Ãµes
4. Avalie a confiabilidade de cada afirmaÃ§Ã£o

Retorne um JSON com este formato:
{{
  "validations": [
    {{
      "claim": "afirmaÃ§Ã£o identificada",
      "is_validated": true/false,
      "confidence": 0.0-1.0,
      "supporting_sources": ["fonte1", "fonte2"],
      "conflicting_info": "descriÃ§Ã£o de conflitos se houver",
      "reasoning": "explicaÃ§Ã£o da validaÃ§Ã£o"
    }}
  ],
  "conflicts_detected": true/false,
  "summary": "resumo da validaÃ§Ã£o"
}}"""

        response = self.llm.invoke([
            SystemMessage(content="VocÃª Ã© um analista de informaÃ§Ãµes que valida fatos cruzando fontes."),
            HumanMessage(content=prompt)
        ])

        # Parse da resposta
        try:
            # Extrai JSON da resposta
            content = response.content.strip()
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()

            validation_data = json.loads(content)

            validations = [
                ValidationResult(**v) for v in validation_data.get('validations', [])
            ]

            conflicts = validation_data.get('conflicts_detected', False)

            print(f"  âœ“ {len(validations)} afirmaÃ§Ãµes validadas")
            if conflicts:
                print(f"  âš ï¸  Conflitos detectados!")

            return {
                "validations": validations,
                "conflicts_detected": conflicts,
                "messages": [validation_data.get('summary', 'ValidaÃ§Ã£o completa')]
            }

        except json.JSONDecodeError as e:
            print(f"  âš ï¸  Erro ao parsear validaÃ§Ã£o: {e}")
            return {
                "validations": [],
                "conflicts_detected": False,
                "messages": ["Erro na validaÃ§Ã£o - usando resposta como texto"],
                "error": str(e)
            }

    def synthesize_report(self, state: ResearchState) -> Dict[str, Any]:
        """
        NÃ³ de sÃ­ntese: cria relatÃ³rio final com referÃªncias
        """
        print(f"\nğŸ“ SINTETIZANDO RELATÃ“RIO FINAL")

        results = state.get('search_results', [])
        validations = state.get('validations', [])

        # Prepara contexto para sÃ­ntese
        sources_summary = "\n\n".join([
            f"FONTE {i+1}: {r.source}\n{r.content[:500]}..."
            for i, r in enumerate(results)
        ])

        validations_summary = "\n".join([
            f"- {v.claim} (confianÃ§a: {v.confidence:.0%})"
            for v in validations
        ])

        prompt = f"""Com base na pesquisa realizada sobre "{state['query']}", crie um relatÃ³rio final completo.

FONTES CONSULTADAS:
{sources_summary}

VALIDAÃ‡Ã•ES:
{validations_summary}

Crie um relatÃ³rio que:
1. Responda diretamente Ã  pergunta original
2. Apresente as informaÃ§Ãµes validadas
3. Mencione incertezas ou conflitos encontrados
4. Cite as fontes apropriadamente
5. Indique o nÃ­vel de confianÃ§a geral

Formato do relatÃ³rio: Markdown profissional"""

        response = self.llm.invoke([
            SystemMessage(content="VocÃª Ã© um pesquisador acadÃªmico que escreve relatÃ³rios claros e bem referenciados."),
            HumanMessage(content=prompt)
        ])

        # Calcula confianÃ§a mÃ©dia
        avg_confidence = sum(v.confidence for v in validations) / len(validations) if validations else 0.5

        # Prepara referÃªncias
        references = [
            {
                "source": r.source,
                "title": r.title,
                "url": r.source
            }
            for r in results
        ]

        print(f"  âœ“ RelatÃ³rio gerado (confianÃ§a: {avg_confidence:.0%})")

        return {
            "final_report": response.content,
            "references": references,
            "confidence_level": avg_confidence,
            "messages": ["SÃ­ntese completa"]
        }

    def decide_next_step(self, state: ResearchState) -> str:
        """
        NÃ³ de decisÃ£o: determina se precisa de mais pesquisa
        """
        current_iter = state.get('current_iteration', 0)
        max_iter = state.get('max_iterations', 1)
        conflicts = state.get('conflicts_detected', False)
        validations = state.get('validations', [])

        # Se tem conflitos e ainda tem iteraÃ§Ãµes disponÃ­veis
        if conflicts and current_iter < max_iter:
            print(f"\nğŸ”„ Conflitos detectados - Nova iteraÃ§Ã£o necessÃ¡ria")
            return "research_more"

        # Se tem poucas validaÃ§Ãµes e ainda pode iterar
        if len(validations) < 3 and current_iter < max_iter:
            print(f"\nğŸ”„ InformaÃ§Ãµes insuficientes - Nova iteraÃ§Ã£o necessÃ¡ria")
            return "research_more"

        print(f"\nâœ… Pesquisa completa - Gerando relatÃ³rio final")
        return "synthesize"